---
date: 2023-11-15
title: 8 集成学习
icon: ensemble_learning
category:
  - 机器学习
tag:
  - 集成学习
---

## 8.1 思想

集成学习(ensemble learning)通过构建并结合多个学习器来提升性能。

::: tip
集成个体应：好而不同。
:::

## 8.2 分类

### 8.2.1 Boosting

从初始训练集训练一个基学习器，根据学习器表现调整样本分布，使得做错的样本得到更多关注，然后基于调整后的分布训练下一个学习器，如此重复直到训练了T个学习器，进行加权结合。

Boosting 族算法最著名的是 AdaBoost。

::: tip
从偏差-方差的角度：降低偏差，可对泛化性能相当弱的学习器构造出很强的集成。
:::

### 8.2.2 Bagging与随机森林

- Bagging：给定数据集，进行m次自助采样得到T个采样集。对每个采样集训练一个学习器，再将学习器结合。预测输出时，采用投票法。

- 随机森林：Bagging 的扩展变体，以决策树为基学习器构建 Bagging 集成。其决策树划分属性时，从当前节点随机选择一个包含 k 个属性的子集，在子集中选择最优属性进行划分。

::: important
随机森林的基学习器多样性来自两个随机性：

1. 构建采样集时采样的随机性
2. 基学习器的属性选择随机性
:::

::: tip
从偏差-方差的角度：降低方差，在不剪枝的决策树、神经网络等易受样本影响的学习器上效果更好。
:::

## 8.3 结合策略

- 平均法：对分类问题，采用投票法；对回归问题，采用简单平均法。

- 投票法：对分类问题，采用简单投票法；对回归问题，采用加权投票法。

- 学习法：基于所有基学习器的预测结果，训练一个元学习器，将其作为最终输出。  


::: note
分析Bagging通常为何难以提升朴素贝叶斯分类器的性能?

- 朴素贝叶斯通过最大化后验概率获得最优分类器。
- Bagging 主要侧重降低方差，但在使用全局训练样本生成朴素贝叶斯分类器中没有方差可以降低，不可能通过随机抽样的方法去提升性能。
:::

::: note
分析随机森林为何比决策树Bagging集成速度要快?

- 随机森林除了样本选择上的随机性，还有划分属性的随机性。
- Bagging 针对所有属性进行属性划分，训练速度更慢。
:::

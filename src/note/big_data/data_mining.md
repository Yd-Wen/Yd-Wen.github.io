---
title: 数据分析与数据挖掘
icon: data_mining
date: 2023-09-15
category: 大数据
tag: 
    - 数据挖掘
    - 决策树
    - 贝叶斯
    - KNN
    - 神经网络
    - 卷积神经网络
    - 模型评估
    - 聚类
    - 检索
    - 推荐系统
---

## 1 绪论

### 1.1 数据挖掘相关概念

- 数据挖掘：从大量的、不完全的、有噪声的、模糊的、随机的实际应用数据中，提取隐含在其中的、人们事先不知道的、但又是潜在有用的信息或知识的非平凡过程。

- 数据挖掘对象：关系数据库、数据仓库、文本、多媒体数据、Web 数据、复杂类型的数据（空间数据库、时间序列数据库）。

- 常用技术：统计技术，关联规则，基于历史的分析，遗传算法，聚集检测，连接分析，决策树，神经网络，粗糙集，模糊集，回归分析，差别分析，概念描述等。

### 1.2 数据挖掘过程

- 明确目标：明确数据分析的对象、目标或任务。

- 搜集数据：规划哪些数据可能影响到问题的答案。

- 清洗数据：数据一致性检验、缺失值和异常值处理、无量纲化处理等。

- 构建模型：构建模型的目的主要是为了预测。

- 模型评估：通过评估不断优化模型使其更好反映数据的真实性。

- 应用部署：部署到系统中提供给业务方或客户。

### 1.3 数据挖掘应用举例

数据挖掘应用最集中的领域包括金融、医疗、教育、零售、电商、电信和交通等，而且每个领域都有特定的应用问题和应用背景。

- 软件工程：可行性和需求分析文档、测试用例和测试结果等。

- 电商：借助交易记录挖出破坏规则的害群之马。

- 交通：为打车平台的乘客订制弹性价格

- 医疗：为病人寻找最佳医疗方案。

## 2 数据预处理

### 2.1 数据清洗

数据清洗试图填充空缺的值、识别孤立点、消除噪声，并纠正数据中的不一
#### 2.1.1 缺失值

含义：数据库表中，记录的对应字段可能没有相应值。

处理方法：

1. **忽略该元组**：将该记录排除在数据挖掘之外。当某类属性的空缺值所占百分比很 大时，直接忽略元组会使挖掘性能变得非常差。

2. **人工填写**：工作量大、可行性低。

3. **属性平均值填充**

4. **全局变量填充**：用常数（如 Unknown）填充。当缺失值较多时，常数也会被认为是一种取值，不可靠。

5. **该元组同类样本的平均值填充**：适用于数据挖掘。

6. **最可能的值填充**：利用回归、贝叶斯等判断最可能的取值。

7. **填充算法处理**：例如采用基于 KNN 算法，选出最近的 K 个数据对应字段平均值填充。

#### 2.1.2 噪声

噪声 (noise): 是一个测量变量中的随机错误或偏差，包括错误的值和偏离期望的孤立点值。

处理方法：

1. 分箱 (binning)：排序数据，并将它们分到等深（等宽） 的箱中,然后可以按箱的平均值、按箱中值或者按箱的边界等进行平滑。

   等深分箱：

![分箱处理-等深分箱](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(1).png)

   等宽分箱：

![分箱处理-等宽分箱](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(2).png)

2. 聚类：通过聚类分析查找孤立点，消除噪声。

#### 2.1.3 不一致数据

处理方法：

- 人工更正

- 利用知识工程工具：例如，直到属性间函数依赖，据此查找违反函数依赖的值。

- 数据字典：根据数据字典信息消除不一致。

### 2.2 数据集成和数据变换

#### 2.2.1 数据集成

数据集成：将来自多个数据源的数据合并。

注意的问题：

- 模式集成：整合不同数据源的元数据，进行实体识别（匹配不同数据源的实体）。

- 数据冗余：属性冗余、记录行冗余。

- 数据值冲突：同一实体来自不同数据源的属性值可能不同，如单位不同。

#### 2.2.2 数据变换

数据变换：对数据进行规范化操作，转换成适合于数据挖掘的形式。

- 平滑：去噪声，连续值离散化。

- 聚集：汇总和聚集数据。

- 数据概化：使用概念分层，用更抽象（更 高层次）的概念来取代低层次或数据层的数据对象。如，街道属性泛化到城市、国家；年龄泛化到年轻、中年、老年等。

- 规范化：缩放数据以消除数值型属性因大小不一造成的数据挖掘偏差。
  - 最小-最大规范化
  - 零-均值规范化
  - 小数定标规范化

- 属性构造：利用已有属性集构造新的属性。

### 2.3 数据规约

数据归约（data reduction）：数据消减或约简，是在不影响最终挖掘结果的前提下，缩小所挖掘数据的规模。

规约策略：

1. 数量规约：用较小数据形式替换原数据，主要方法有回归、聚类、采用等。

2. 数据压缩：使用变换，以便得到原数据的归约或“压缩”表示。

3. 维规约：减少所考虑的随机变量或属性的个 数。 

规约标准：

- 规约时间不超过或抵消规约后数据集中挖掘时间哎。

- 归约得到的数据比原数据小得多，但分析结果几乎相同。

## 3 认识数据

### 3.1 数据对象与属性类型

数据集：数据集是数据挖掘的对象，由数据对象组成， 又称样本、实例、数据点或元组。

属性类型：

- 标称属性：常见的二元属性。只有两个类别或状态。0 或 1，其中 0 常表示不出现，1 表示出现。

- 序数属性：优、良、中、差四个等级。

- 数值属性：数值属性是可度量的量，用整数或实数值表示 ，有区间标度和比率标度两种类型

- 离散属性与连续属性

### 3.2 数据的基本统计描述

#### 3.2.1 数据中心趋势度量

- 均值：数据是平均值

  中位数：是常用的数据中心度量，是有序数据值的中间值

  众数：是集合中出现最频繁的值

#### 3.2.2 度量数据分布

1. 极差、四分位数和四分位数极差：极差：数据是最大

2. 方差与标准差

3. 协方差与协方差矩阵

### 3.3 度量数据的相似性和相异性

**明考夫斯基距离的缺陷**:

容易受变量的量纲影响； 

没有考虑变量间的相关性。

**两种改进措施**：

“马氏距离”法变量：考虑了观测变量之间的相关性，不再受各指标量纲的影响。

标准化处理。

## 4 决策树分类算法

### 4.1 基本思想

- 训练阶段：从给定的训练集构造出来一棵树 (从根节点开始选择特征，如何进行特征切分)

- 测试阶段：根据构造出来的树模型从上到下去走一遍就好了

- 切分特征（选择节点）：通过一种衡量标准度量特征切分后的情况，找出最好的那个属性作为根节点。

- 衡量标准：信息增益（表示特征 X 使得类 Y 的不确定性减少的程度）。

### 4.2 两类决策树方法

研究结果表明，一般情况下，树越小则树的预测能力越强。要构造尽可能小的决策树，关键在于选择合适的产生分支的属性。

由于构造最小的树是 NP-难问题，因此只能采用启发式策略来进行属性选择。

属性选择依赖于对各种样本子集的不纯度度量方法。不纯度度量方法包括信息增益、信息增益率（比）、距离度量、Gini 系数等。

#### 4.2.1 ID3 决策树

以信息增益为衡量标准。

**缺点**：

- 信息增益倾向于选择取值较多的属性。

- 只能对描述属性为离散型属性的数据集构造决策树。

- 单变量决策树，复杂概念表达困难。

- 抗噪性差，训练样本中正反例的比例较难控制。

#### 4.2.2 C4.5 决策树

以信息增益率为衡量标准，解决 ID3 的问题。

## 5 贝叶斯分类算法

### 5.1 朴素贝叶斯

满足条件独立性假设，即各属性独立的贝叶斯分类器.

![朴素贝叶斯公式](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(3).png)

### 5.2 平滑

用极大似然估计可能会出现所要估计的概率值为 0 的情况，这时会影响到后验概率的计算结果 ，使分类产生偏差。

解决方法是拉普拉斯平滑：

![拉普拉斯平滑公式](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(4).png)

### 5.3 特点与应用

#### 5.3.1 优点

- 逻辑简单，易于实现，开销较小

- 算法稳定，有较好的健壮性

#### 5.3.2 缺点

条件独立假设很多实际问题中并不成立，导致分类效果下降。

#### 5.3.3 应用

图像识别、文本分类等。

## 6 KNN 算法

### 6.1 原理

基本思想：

1. 度量距离。根据距离函数计算待分类样本 X 和每个训练样本的距离（作为相似度）。

2. 选出 K 近邻。选择与待分类样本距离最小的 K 个样本作为 X 的 K 个最邻近。

3. 归类。最后以 X 的 K 个最邻近中的大多数所属的类别作为 X 的类别。

实现步骤：

1. 初始化距离为最大值；

2. 计算未知样本和每个训练样本的距离 dist； 

3. 得到目前 K 个最临近样本中的最大距离 maxdist；

4. 如果 dist 小于 maxdist，则将该训练样本作为 K- 最近邻样本； 

5. 重复步骤 2、3、4，直到所有未知样本和所有训练样本的距离都算完； 

6. 统计 K-最近邻样本中每个类标号出现的次数； 

7. 选择出现频率最大的类标号作为未知样本的类标号。

### 6.2 特点与应用

优点：

1. 简单易于实现

2. 适合对稀有事件分类

3. 适合多分类问题

缺点：

1. 分类速度慢

2. 各属性权重相同，影响准确率

3. 样本库容量依赖性较强

4. K 值不好确定，过小降低分类精度，放大噪声干扰；过大包含不相似数据，噪声增加，分类效果降低。

应用例子：

- 个性化推荐系统。根据用户行为和偏好，使用 KNN 找到具有相似兴趣的其他用户。

- 图形识别。将图像表征为特征向量。使用 KNN 识别。

### 6.3 改进策略

1. 降低计算复杂度：对样本属性进行约简。

2. 优化相似度度量方法：距离公式中给特征赋予不同权重。

3. 优化判决策略：类别不平衡时，容易误判，采用**均匀化样本分布密度**的方法。

4. 选取 K 值：反复试验调整 K 值。

## 7 人工神经网络

### 7.1 相关概念

常见三种神经网络结构：

- 前馈神经网络：整个网络中的信息是朝着一个方向传播的，没有反向的信息传播。包括全连接前馈神经网络和卷积神经网络。

- 反馈神经网络：可以接收自己的反馈信号，其中的神经元具有记忆功能。包括循环神经网络和玻尔兹曼机。

- 图网络：定义在图结构数据上的神经网络。图中每个结点都由一个或者一组神经元组成。结点之前的连接可以是有向的，也可以是无向的。每个结点可以收到来自相邻结点或自身的信息。

::: tip
BP (back propagation) 神经网络是 1986 年由 Rumelhart 和 McClelland 为首的科学家提出的概念，是一种按照**误差逆向传播算法**训练的**多层前馈神经网络**，是应用最广泛的神经网络模型之一.
:::

### 7.2 感知机

感知机由两层组成：

输入层：接收外界信号并将其传递给输出层输出层。 

输出层：该层由一个 M-P 神经元组成，对输入层传递的信号进行计算，并根据计算结果对输入信号的性质进行判别。

## 8 卷积神经网络

### 8.1 概述

神经网络发展（三次兴起）：

1. 第一次兴起（1958）：感知机，无法求解异或问题。

2. 第二次兴起（1986）：BP 算法用于人工神经网络训练。

3. 第三次兴起（2012）：深度卷积神经网络兴起，直到现在。

常用深度神经网络模型分类：

- CNN：卷积神经网络

- RNN：循环神经网络

- GAN：生成式对抗网络

- GNN：图神经网络

CNN 结构：

输入层、隐藏层、输出层构成。其中隐藏层又包括卷积层、池化层、全连接层。

![CNN 结构](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(5).png)

### 8.2 CNN 各层介绍

#### 8.2.1 卷积层

##### 1. 优势

图像分类任务的瓶颈出现在**特征提取**上。

卷积的优势：

1. 稀疏连接：每个神经元感受局部图像区域。

2. 参数共享：卷积核充当共享感受野的角色。   

##### 2. 填充

卷积后矩阵越来越小，处理方法：填充（padding）。

无填充的缺点：

- 卷积后图像缩小

- 角落像素只于卷积核操作了一次

::: tip
使用填充后，卷积后大小不会丢失；卷积核边长（k）是**奇数**时，特征图与原图大小相等。
:::

##### 3. 步长

步长（stride）：卷积核在输入数据上滑动的步长大小。它决定了卷积核在每次滑动时移动的距离。

较大步长：可以减小输出特征图的尺寸，可能导致信息丢失。

较小步长：可以保持输出特征图的尺寸更接近输入数据的尺寸，保留更多的空间信息，但会增加计算量和内存消耗。

输入输出计算公式：

O = ( I - K + 2P ) / S + 1

::: tip
O：输出

I：输入

K：卷积核大小

P：填充

S：步长
:::

##### 4. 激活函数

对输入进行卷积运算得到特征图后，往往需要使用激活函数对特征图进行激活。

ReLU 激活函数优势：

- 反向传播时避免梯度消失

- 使部分神经元输出为 0，减少参数依存关系，缓解过拟合。

- 求导简单，整个过程计算量节省很多。

#### 8.2.2 池化层

池化层（也称下采样层）旨在通过降低特征面的分辨率来获得具有尺度不变性的特征。池化层起到二次提取特征的作用，它的每个神经元对局部接受域进行池化操作。

分类：

- 最大池化

- 平均池化

作用：

1. 减少参数量，提高计算效率

2. 提高局部平移不变性

3. 降低数据维度，避免过拟合

4. 增强网络对输入图像的鲁棒性

#### 8.2.3 全连接层

全连接层中的每个神经元与其前一层的所有神经元进行全连接。

全连接层可以整合卷积层或者池化层中具有类别区分性的局部信息。

为了提升 CNN 网络性能，全连接层每个神经元的激励函数一般采用 ReLU 函数。

最后一层全连接层的输出值被传递给一个输出层，可以采用 softmax 逻辑回归进行分类，该层也可称为 softmax 层。

![全连接层](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(6).png)

### 8.3 CNN 各层作用

卷积层：**局部特征提取**。

池化层：降低数据维度，避免过拟合；增强**局部感受野**；提高平移不变性。

全连接层：特征加工，**映射**到输出类别。

激励层：引入**非线性**变换以增强网络的表达能力。

### 8.4 总结

#### 8.4.1 特点

1. 局部感知性：CNN 利用卷积层和池化层的操作，可以有效地捕捉输入数据的局部空间特征。

2. 参数共享：在 CNN 中，卷积核的参数被共享，使得网络可以在不同位置上共享特征提取器，减少模型的参数数量。

3. 平移不变性：通过卷积操作，即对于相同的特征，无论其出现在图像的哪个位置，都可以被识别出来。

#### 8.4.2 训练和学习过程

1. 初始化：随机初始化网络的权重和偏置。

2. 前向传播：将输入数据通过网络的卷积层、池化层和全连接层进行前向传播，得到输出结果。

3. 损失计算：比较输出结果和真实标签之间的差异，计算损失函数。

4. 反向传播：根据损失函数，通过反向传播算法计算网络参数的梯度。

5. 参数更新：使用优化算法（如梯度下降），根据参数的梯度更新网络的权重和偏置。

6. 重复步骤 2-5，直到达到指定的停止条件（如达到最大迭代次数或损失函数收敛）。

#### 8.4.3 优缺点

优点：

1. 对于图像和视觉任务具有良好的表现力和特征提取能力。

2. 能够自动学习和提取高级特征，避免手工设计特征。

3. 具有参数共享和平移不变性的特性，使得模型更加高效和鲁棒。

缺点：

1. 需要大量的训练数据和计算资源，尤其是在较复杂的任务和大型数据集上。

2. 容易出现过拟合，需要适当的正则化方法和数据增强技术来提高模型的泛化能力。

3. 对于输入尺寸的变化较敏感，可能需要额外的预处理步骤（如图像缩放或裁剪）来保持输入数据的一致性。

## 9 模型评估

### 9.1 常用术语

真正率（灵敏度 / 查全率）：TPR = TP / ( TP + FN )

假反率：FNR = FN / ( TP + FN )

假正率：FPR = FP / ( FP + TN )

真反率：TNR = TN / ( TN + FP )

### 9.2 评价指标

1. 正确率：accuracy = （TP+TN）/(P+N)

2. 错误率：error rate = (FP+FN)/(P+N)

3. 灵敏度（真正率 / 查全率）：sensitive / TPR = TP / ( TP + FN )

4. 特效度（真反率）：specificity / TNR = TN / ( TN + FP )

5. 精度（查准率）：precision=TP / （ TP + FP ）

6. 召回率：灵敏度

### 9.3 分类器性能评估方法

1. 保持法：留出法

2. 交叉验证法（K = M 留一法）

## 10 聚类分析

### 10.1 概述

目标：

聚类分析的目标就是形成多个数据簇，并且数据簇需要满足下面两个条件：同一个簇内的数据尽量相似；不同簇的数据尽量不相似。

应用：商务应用中对目标用户群体进行划分；万维网上对用户使用情况进行聚类，挖掘出社区。

分类：

- 基于划分的聚类算法

- 基于层次的聚类算法

- 基于密度的聚类算法

- 基于概率的聚类算法

- 基于图和网络的聚类方法

### 10.2 基于划分的聚类方法

典型划分方法有 k-means（k-均值）算法、k-medoids（k-中心的）算法。

#### 10.2.1 K-means 聚类算法

算法流程：

1. 初始化 K 值和聚类中心

2. 计算样本到各个聚类中心的距离

3. 比较距离，将样本划入相应簇中

4. 计算新的均值向量 

5. 判断聚类中心是否变化，变换则重复 2-4 步，否则输出聚类结果

优点：

- 简单快速

- 处理大数据集相对高效

缺点：

- 不适合处理离散属性

- 事先指定 K，对初始值敏感

- 对噪声和孤立点敏感（影响均值）

应用：散货船代货运方面的航线繁忙度的分析，得出航线繁忙度分析结果的意义

#### 10.2.2 K-medoids 聚类算法

K-means 方法对于离群点敏感，一个极端值可能扭曲数据分布。使用 K-medoids，采用最靠近中心的对象来代表簇，可以降低算法对离群点的敏感度。

更新聚类中心的流程：

假设原来的聚类中心是 A 和 B，即{A, C, D}，{B, E}

![K-medoids 聚类](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(7).png)

计算替换中心后的各个点的损失并求和。

![K-medoids 损失计算](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(8).png)

损失低的中心作为新的中心。

优点：

- 对噪声、孤立点不敏感

- 聚类结果具有数据对象平移和正交变换不变性

缺点：迭代寻找最佳聚类中心，聚类过程缓慢，耗时高。

应用：暂住人口挖掘，发现不同特征的暂住人群。

### 10.3 基于密度的聚类方法

基于距离的聚类方法的缺点：只能发现球状的簇，难以发现任意形状的簇。

基于密度的聚类：只要临近区域的密度（对象或数据点的数目）超过某个临界值，就继续聚类。

常见聚类方法：DBSCAN（具有噪声的基于密度的聚类算法）。

概念：

1. 核心对象：样本邻域至少包含 MinPts 个样本。

2. 密度直达：样本在核心对象的邻域中。

3. 密度可达：传递性密度直达。

4. 密度相连：存在一个样本使得另外两个样本与该样本密度可达。称这两个样本密度相连。

5. 核心点：邻域包含至少 MinPts 个样本。

6. 边界点：不是核心点，但是其邻域内包含至少一个核心点。

7. 噪声点：不是核心点，也不是边界点。

优点：

- 速度快

- 有效处理噪声

- 发现任意形状的空间聚类

缺点：

- 数据量大时开销大

- 密度不均匀时，聚类质量差

- 对初始参数（邻域半径和最小点数）敏感

## 11 文本检索技术

### 11.1 词频-文档的关联矩阵

![词频-文档关联矩阵](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(9).png)

对关联向量进行按位与操作（注意 NOT 对象求补）。

![按位与操作](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(10).png)

### 11.2 TF-IDF

TF（词频）表示词语在文本中出现的频率，计算公式如下：TF(t, d) = (词语 t 在文本 d 中出现的次数) / (文本 d 的总词数)

IDF（逆文档频率）表示词语在整个文档集合中的普遍程度，计算公式如下：IDF(t, D) = log((文档集合 D 的总文档数) / (包含词语 t 的文档数 + 1))

最后，TF-IDF 的计算公式为：TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)

## 12 关联规则挖掘

### 12.1 基本概念

事务集、项集

支持度：项集 A 在事务数据库 D 中出现的次数占 D 中总事务的百分比叫做项集的支持度。

频繁项集（频集）：如果项集的支持度超过用户给定的最小支持度阈值，就称该项集是频繁项集（或频集）。

支持度：support(X⇒Y)=P(X∪Y) 

信任度：confidence(X⇒Y)=P(Y|X)

提升度：

lift(X⇒Y)=confidence(X⇒Y) / support(Y)

![关联规则计算](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(11).png)

![关联规则示例](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(12).png)

![关联规则结果](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(13).png)

两个定理：

![关联规则定理](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(14).png)

强关联规则：支持度和信任度满足用户给定阈值的规则。

关联规则挖掘的步骤：

1. 找出所有频繁项集。

2. 由频繁项集生成满足最小信任度阈值的规则。

### 12.2 关联规则分类

1. 基于规则中处理的变量的类别：例如：性别=“女”⇒职业=“秘书”，是布尔型关联规则；性别=“女”⇒收入=2300，涉及的收入是数值类型，所以是一个数值型关联规则。

2. 基于规则中数据的抽象层次：A 打印机⇒B 打印机是单层关联规则；打印机⇒B 打印机是多层关联规则。

3. 基于规则中涉及到的数据的维度：啤酒⇒尿布，这条规则只涉及到用户的购买的物品；性别=“女”⇒职业=“秘书”，这条规则就涉及到两个字段的信息，是两个维上的一条关联规则。

### 12.3 Apriori 算法

两条性质：

性质 1：频繁项集的子集必为频繁项集。 

性质 2：非频繁项集的超集一定是非频繁的。

算法流程：

找到满足阈值的频繁 K 项集。

![频繁项集](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(15).png)

获取其真子集。

![真子集](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(16).png)

以排列组合形式生成关联规则，计算置信度和提升度。

![关联规则生成](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(17).png)

优点：简单易于实现。

缺点：

- 时间复杂度大

- 采用唯一支持度，未考虑各个属性重要程度的不同

## 13 推荐系统概述

### 13.1 相关概念

搜索引擎与推荐系统的异同点：

相同点：都是帮助用户快速发现有用信息的工具。

不同点：搜索引擎需要用户主动提供关键词来寻找信息；推荐系统不需要用户提供需求，而是通过分析用户的历史行为给用户的兴趣建模。

二者是两个互补的工具。

### 13.2 协同过滤

分类：

- 基于近邻方法：基于用户推荐；基于物品推荐。

- 基于模型方法：基于用户推荐；基于物品推荐。

协同过滤：依赖于用户对物品的评分。

基于用户的协同过滤：

1. 获取用户向量

2. 计算目标用户与其余用户余弦相似性

3. 计算最终预测结果

![用户协同过滤](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(18).png)

方法一：

![用户协同过滤方法一](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(19).png)

方法二：

![用户协同过滤方法二](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(20).png)

基于物品的协同过滤：

1. 获取物品向量

2. 计算目标物品与其余物品余弦相似性

3. 计算最终预测结果

![物品协同过滤](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(21).png)

![物品协同过滤计算](https://gitee.com/yindong-wen/picgo_img/raw/master/image/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%20(22).png)

---
date: 2025-01-23
title: 8 模型评估
icon: evaluation
category:
  - 深度学习
tag:
  - 模型评估
  - 数据集划分
  - Dropout
  - BatchNorm
  - ResNet
  - DenseNet
---

## 8.1 数据集划分

### 8.1.1 验证数据集和测试数据集

验证数据集：

- 用来评估模型好坏的数据集

- 如拿出50%训练数据

- 用于调参

::: important
**不要跟训练数据混在一起**（常犯错误）
:::

测试数据集:

- 只用一次的数据集

- 用于测试泛化能力

验证集和测试集对比：参考[验证集和测试集的区别](https://blog.csdn.net/weixin_44211968/article/details/120814758)。

![验证集和测试集对比.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/202503261027934.png)

在传统的机器学习中，这三者一般的比例为 training/validation/test = 50/25/25, 但是有些时候如果模型不需要很多调整只要拟合就可时，或者 training 本身就是 training + validation (比如cross validation)时，也可以 training/test =7/3.

但是在深度学习中，由于数据量本身很大，而且训练神经网络需要的数据很多，可以把更多的数据分给training，而相应减少validation和test。

::: important
- 验证数据集和测试数据集都不能用来训练模型参数
- 验证数据集和测试数据集都不能用来调整模型超参数
:::

### 8.1.2 K-则交叉验证

- 没有足够多数据时使用
- 算法：
	- 训练数据分割成 K 块
	- For i=1,...,K
		- 使用第 i 块作为验证集，其余作为训练集
	- 报告 K 个验证集的平均误差
- 常用：K=5或10

更多数据集划分方法参考：

- [机器学习：模型评估-数据集划分方法](../machine_learning/evaluation.md#2-2-数据集划分方法)


## 8.2 训练误差与泛化误差

训练误差：模型在训练数据集上的误差，**用于训练模型参数**。

泛化误差：模型在新数据集上的误差，**用于选择模型超参数**。

::: note
例子：根据模拟考试成绩预测未来考试分数
- 过去考试表现好（训练误差）不代表未来考试一定好（泛化误差）
- 学生A背书
- 学生B知道答案背后的原因
:::

## 8.3 过拟合与欠拟合

模型容量：

- 拟合各种函数的能力

- 低容量模型难以拟合训练数据

- 高容量模型可以记住所有训练数据

![模型容量.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/202503261020122.png)

模型容量的影响：

![模型容量的影响.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/202503261021845.png)

数据复杂度：

- 样本数量

- 每个样本的元素个数

- 时间空间结构

- 多样性

参考：

- [机器学习：模型评估-三种误差与两种拟合](../machine_learning/evaluation.md#2-1-三种误差与两种拟合)

## 8.4 模型优化

### 8.4.1 丢弃法(Dropout)

动机：一个好的模型需要对输入数据的扰动鲁棒。

方法：

- 在层之间加入噪声

- 对每个元素进行如下扰动
$$
x^{'}_i = 
\begin{cases}
0& \text{with probablity p} \\
\frac{x_i}{1-p}& \text{otherise}
\end{cases}
$$

- 通常作用在隐藏全连接层的输出上
$$
\begin{align}
h &= \sigma(W_1x+b_1) \\
h^{'} &= dropout(h) \\
o &= W_2h^{'}+b_2 \\
y &= softmax(o)
\end{align}
$$

![丢弃法.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/dl_10_4_01.png)

- 推理：直接返回输入，保证确定性的输出
$$
h = dropout(h)
$$

::: tip
- 丢弃法通过一些输出项随机置0来控制模型复杂度

- 常作用在多层感知机的隐藏层输出上

- **丢弃概率**是控制模型复杂度的**超参数**

- 在 RNN（GRU/LSTM） 中，`dropout` 通常被应用在每一层（除了最后一层）的输出上，此时需要设置`num_layer>1`，否则`dropout`不生效
:::

### 8.4.2 批量归一化(BatchNorm)

批量归一化

- 引入

	- 损失在最后，后面的层训练较快

	- 数据在最底部

		- 底部的层训练较慢

		- 底部的层一旦变化，所有都得跟着变化

		- 最后的那些层需要重新学习多次

		- 导致收敛变慢

	- 如何在学习底部层的时候避免变化顶部层

- 操作

	- 固定小批量里面的均值和方差

$$
\begin{align}
\mu_B&=\frac{1}{|B|}\sum_{i\in B}x_i \\
\sigma_B^2&=\frac{1}{|B|}\sum_{i\in B}(x_i-\mu_B)^2+\epsilon
\end{align}
$$

  - 然后再作额外调整（可学习参数）

$$
x_{i+1}=\gamma\frac{x_i-\mu_B}{\sigma_B}+\beta
$$
	
  - 可学习参数：$\gamma$ 和 $\beta$

- 作用于：

	- 全连接层和卷积层输出后，激活函数前

	- 全连接层和卷积层输入前

	- 对于全连接层，作用在特征维

	- 对于卷积层，作用在通道维

- 本质

	- 通过在每个小批量加入噪音来控制模型复杂度

	- 因此没必要跟丢弃法混合使用

	- 固定小批量中的均值和方差，然后学习出合适的偏移$\beta$和缩放$\gamma$

	- 可以加速收敛速度（可设置较大的学习率），但一般不改变模型精度

- 示例

```python
net = nn.Sequential(
	nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),
	nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
	nn.Conv2d(6, 16, kernel_size=5), nn.BatchNorm2d(16),
	nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
	nn.Flatten(),
	nn.Linear(256, 120), nn.BatchNorm2d(120),nn.Sigmoid(),
	nn.Linear(120, 84), nn.BatchNorm2d(84),nn.Sigmoid(),
	nn.Linear(84, 10)	
)
```

### 8.4.3 残差网络(ResNet)

网络加深总是改进精度吗？

残差块:

- 串联一个层改变函数类，我们希望能扩大函数类

- 残差块加入快速通道得到$f(x)=x+g(x)$的结构

- 使得很深的网络更容易训练

- 对深层神经网络产生深远影响

### 8.4.4 密集网络(DenseNet)

密集连接（Dense Connectivity）与残差连接（Residual Connectivity）是两种不同的网络连接模式，核心区别主要体现在连接方式、特征组合方式及网络特性上。具体如下：

1. 连接方式与特征组合

	- **残差连接（ResNets）**：第 $\ell$ 层的输入仅来自第 $\ell -1$ 层，通过“跳跃连接”将前一层的输出与当前层的非线性变换结果相加，公式为 $(x_\ell=H_\ell(x_{\ell -1}))+x_{\ell-1}$，其中 $H_\ell(\cdot)$ 为非线性变换（如卷积、批归一化等）。这种方式通过**加法**融合特征，仅在相邻层间建立直接连接，整个L层网络仅有L个连接。
	
	- **密集连接（DenseNets）**：第 $\ell$ 层的输入是所有前面层 $(0, 1, \dots, \ell -1)$ 的特征图，通过**拼接**而非求和的方式组合特征，公式为 $x_\ell=H_\ell([x_0, x_1, \dots, x_{\ell-1}])$，其中 $[x_0, x_1, \dots, x_{\ell-1}]$ 表示特征图的拼接操作。这种方式在所有前面层与当前层间建立直接连接，L 层网络的连接数为 $\frac{L(L-1)}{2}$，远多于残差连接。

2. 特征重用与参数效率

	- **残差连接**：虽通过跳跃连接缓解了梯度消失问题，但每层仍需学习冗余特征，参数数量较多。例如，1001 层的 ResNet 参数达 10.2M。
	
	- **密集连接**：通过拼接保留了所有前面层的特征，无需重新学习冗余信息，因此参数效率更高。例如，100 层的 DenseNet-BC 仅需 0.8M 参数，即可达到与 1001 层 ResNet 相当的性能。

3. 网络训练与优化

	- **残差连接**：依赖加法融合特征，可能导致特征信息稀释；深层网络需通过随机深度（随机丢弃层）优化训练。
	
	- **密集连接**：拼接操作保留了更丰富的特征信息，使每层能直接获取原始输入和损失函数的梯度，缓解了梯度消失问题，训练更稳定，且无需随机丢弃层即可训练极深网络。

4. 模型紧凑性

	- **残差连接**：网络状态通过层间传递并不断修改，每层需保留必要信息，导致模型相对冗余。
	
	- **密集连接**：将特征图视为网络的“集体知识”，每层仅添加少量新特征（由增长率k控制），模型更紧凑，且不易过拟合。

综上，密集连接通过更密集的特征交互和高效的特征重用，在参数效率、训练稳定性和模型紧凑性上表现更优，而残差连接则以更简洁的相邻层连接实现了深层网络的训练突破。

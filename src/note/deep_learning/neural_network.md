---
date: 2025-01-12
title: 7 神经网络
icon: neural_network
category:
  - 深度学习
tag:
  - 神经网络
  - 激活函数
---

## 7.1 简介

**人工神经网络**(Artificial Neural Network, ANN)，简称**神经网络**(Neural Network, NN)或**类神经网络**，是一种模仿生物神经网络的结构和功能的数学模型，用于对函数进行估计或近似。

和其他机器学习方法一样，神经网络用于解决各种各样的问题，例如机器视觉和语音识别，这些问题很难用传统基于规则的编程所解决。

## 7.2 神经元

神经网络基础单元，相互连接组成神经网络。由沃伦·麦卡洛克（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）在 1943 年提出的理论模型。

一个简单的神经元如下图所示：

![神经元.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/dl_5.2.png)

其中：
- $a_1$, $a_2$, ... $a_n$ 为各个输入分量。
- $w_1$, $w_2$, ...$w_n$ 为各个输入分量对应的权重参数。
- $b$ 为偏置。
- $f$ 为激活函数，常见激活函数有 $tanh$, $sigmoid$, $relu$。
- $t$ 为神经元输出。

使用数学公式表示：
$$
t = f(W^TA + b)
$$

可见，一个神经元的**功能**就是：**求得输入向量与权重向量的内积后，经一个非线性激活函数得到一个标量结果**。

## 7.3 单层神经网络

最简单的神经网络形式，由有限个神经元构成，所有神经元输入向量都是同一个向量。由于每一个神经元都会产生一个**标量**结果，所以单层神经网络的输出就是一个**向量**。向量的维数等于神经元的个数。

## 7.4 感知机

感知机由**两层神经网络**组成，**输入层**接收外界输入信号后传递给**输出层**（输出+1正例，-1负例），输出层是M-P神经元。
![感知机.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/dl_5.4.png)

作用：
把一个 n 维向量空间用一个超平面分割成两部分，给定一个输入向量，超平面可以判断这个向量位于超平面的哪一边，得到输入时输出正例或负例，对应到 2 维空间就是一条直线把一个平面分为两个部分。

::: note
- 感知机是二分类模型，是最早的 AI 模型之一
- 求解算法等价于使用批量大小为 1 的梯度下降
- 不能拟合 XOR 函数，导致第一次 AI 寒冬
:::

## 7.5 多层神经网络 

多层神经网络就是由单层神经网络进行叠加后得到的，所以就形成了层的概念。**层数通常是指可学习的权重数量**。

常见的多层神经网络有如下结构：

- **输入层**(*Input layer*)：众多神经元接收大量输入消息，输入消息称为输入向量。

- **输出层**(*Outout layer*)：消息在神经元链接中传输、分析、权衡，形成输出结果，输出的消息称为输出向量。

- **隐藏层**(*Hidden layer*)：简称“隐层”，是输入层和输出层之间众多神经元和链接组成的各个层，可以有一层或多层，隐层的节点（神经元）数目不定，但数目越多神经网络的非线性越显著，从而神经网络的**强健性/鲁棒性***(robustness*)更显著。

![多层神经网络.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/dl_5.5_01.png)

::: note
隐藏层的大小是**超参数**。
:::

- **全连接层**：第N层的每个神经元和第N-1层每个神经元都相互链接，则称第N层为全连接层。

::: tip
若第 N 层有 $n$ 个神经元，第 N-1 层有 $m$ 个神经元，则两层之间的权重参数个数为 $nm$。
:::

![全连接层.png](https://gitee.com/yindong-wen/mypicgo_img/raw/master/image/dl_5.5_02.png)

上图可以看出，全连接层的作用就是：

在前一层的基础上进行一次 $Y=Wx+b$ 的变换。不考虑激活函数的情况下，就是一次线性变换。线性变换就是平移($+b$)和缩放($*W$)的组合。

::: tip
线性层仅对最后一个维度进行变换：
- **二维输入**（最常见场景）：
  - **输入形状**：`[batch_size, in_features]`
  - **输出形状**：`[batch_size, out_features]`
- **多维输入**（如序列、图像特征）：
  - **输入形状**：`[batch_size, seq_len, in_features]` 或 `[batch_size, channels, height, width]`
  - **输出形状**：`[batch_size, seq_len, out_features]` 或 `[batch_size, channels, height, out_features]`
:::

## 7.6 激活函数

没有激活函数时，即使是多层神经网络，相比于感知机，没有任何改进，仍然只能进行线性划分，会导致**层数塌陷**。但是在感知机基础上加上**非线性激活函数**后，输出结果不再是一条直线。

激活函数的作用就是：

- **增强模型的非线性分割能力**；

- 提高模型稳健性；

- 缓解梯度消失；

- 加速模型收敛。

常见激活函数有：$sigmoid$,  $ReLu$, $tanh$,  $ELU$，其中，$sigmoid$,   $tanh$ 都需要做指数运算，计算慢。实际工程中激活函数不能提高模型表现，推荐使用 $ReLu$。
